短文本情感三分类项目 README

Sentiment-Classification-with-BERT-BiLSTM-TextCNN-and-LoRA

本项目聚焦短文本情感三分类任务，复现了论文《BERT-BiLSTM-TextCNN 模型的网民情感分类研究》的核心融合架构，并针对 Kaggle 等资源受限环境，引入 LoRA 低秩适配微调技术进行优化，实现了「高准确率+低参数量+快训练速度」的平衡，为网络舆论分析、网民心理洞察提供技术支撑。

📋 项目简介

1.1 研究任务

对社交媒体（以微博为例）中的网民评论进行情感极性判定，精准区分「喜悦、愤怒、厌恶」三类情感（基于数据集去重后简化）。

核心目标：在适配 Kaggle 等资源受限环境的前提下，保证分类准确率，实现模型高效训练与部署。

1.2 数据集

- 原始数据集：simplifyweibo_4_moods（含 36 万+条带情感标注的新浪微博，原始 4 类情感）

- 数据预处理：去重（剔除「厌恶」与「低落」类重复文本）、均衡抽样（每类 15508 条）

- 数据集划分：按 7:2:1 划分训练集/验证集/测试集，分别保存为 train_set.csv、val_set.csv、test_set.csv

1.3 核心改进

针对原论文 BERT-BiLSTM-TextCNN 架构全量微调算力需求高的问题，引入 LoRA（Low-Rank Adaptation）低秩适配技术：

- 冻结 BERT 预训练模型主干，仅训练插入核心模块的低秩适配器参数（可训练参数占比不足 5%）

- 结合 L2=1 强正则化策略，平衡模型性能与过拟合风险

- 适配资源受限环境：训练效率提升，显存占用从 8.2GB 降至 4.5GB 左右

🔍 相关研究与改进动机

2.1 主流方案对比

方案类型

代表模型

优势

缺陷

单一神经网络

TextCNN、BiLSTM

TextCNN 训练快、抓局部特征；BiLSTM 抓上下文依赖

TextCNN 缺语义依赖；BiLSTM 训练慢、缺局部特征；传统词嵌入无动态语义

混合神经网络

BERT-BiLSTM-TextCNN

融合「语义向量+时序特征+局部特征」，分类准、稳定性强（原论文 Micro-F1/Macro-F1 > 0.9）

全量微调参数量大（超 1 亿）、算力需求高，Kaggle 环境训练耗时久（>2.5 小时）

2.2 改进动机

1. 适配资源受限环境：解决原架构全量微调高算力与 Kaggle 平台约束的矛盾

2. 保留核心架构优势：无需重构，仅优化训练方式即可复用「三重特征融合」的优势

3. 平衡性能与效率：实现「高准确率+低参数量+快训练速度」，满足实际部署需求

4. 验证技术适配性：填补 LoRA 在 BERT-BiLSTM-TextCNN 混合架构中应用的实践空白

⚙️ 项目实现方案

3.1 任务形式化描述

- 输入：预处理后的微博短文本集合 X = {x₁, x₂, ..., xₙ}（xᵢ 为词元序列）

- 输出：情感标签集合 Y = {y₁, y₂, ..., yₙ}（标签空间 C = {0:喜悦, 1:愤怒, 2:厌恶}）

- 模型映射：f: X→Y，将文本映射为三类情感的概率分布（满足概率归一化）

- 损失函数：交叉熵损失函数，衡量预测分布与真实标签的差异

3.2 实验环境

- 硬件：Kaggle T4 GPU

- 框架：PyTorch

- 核心依赖：transformers（BERT 预训练模型）、peft（LoRA 微调）、pandas、numpy、matplotlib（可视化）

3.3 模型架构

核心架构：BERT+LoRA → BiLSTM → TextCNN → 分类层

1. BERT+LoRA 嵌入层：
       

  - BERT 生成 768 维上下文语义向量（token+segment+position 嵌入）

  - LoRA 配置：r=16（低秩维度）、lora_alpha=64（缩放因子）、dropout=0.05、任务类型=特征提取

2. BiLSTM 层：单层双向 LSTM（隐藏层维度 300），捕捉时序依赖特征，输出 600 维向量

3. TextCNN 层：多尺度卷积核（3/4/5）+ 最大池化，提取局部关键词特征，输出 300 维向量

4. 分类层：全连接层融合时序+局部特征，输出三类情感概率

3.4 训练配置

- 批量大小：300

- 优化器：AdamW（分层学习率：LoRA 层 2e-3，其他层 1e-3）

- 训练策略：混合精度训练 + 梯度累积（4 步）

- 学习率调度：余弦退火（暖启动占比 10%）

- 训练轮次：30 轮

📊 实验结果

4.1 评估指标

采用 Precision（精确率）、Recall（召回率）、F1 值、Micro-F1、Macro-F1 综合评估，兼顾整体准确率与小众类别分类能力。

4.2 核心结果

- 测试集性能：Micro-F1 = 0.5141，Macro-F1 = 0.5140

- 类别级性能：喜悦类 F1 = 0.5628，愤怒类 F1 = 0.4926，厌恶类 F1 = 0.4866

- 训练效率：Kaggle T4 GPU 上 30 轮训练总耗时远低于原论文全量微调方案，显存占用降至 4.5GB

- 收敛情况：训练损失从 1.7652 降至 0.8481，训练集 Micro-F1 从 0.3356 提升至 0.6171

4.3 与原论文对比

对比维度

原论文

本项目

任务类型

四分类（喜悦、愤怒、厌恶、低落）

三分类（去重后保留三类）

训练范式

全量微调 BERT

LoRA 轻量化微调（冻结主干）

核心性能

Micro-F1/Macro-F1 > 0.9

Micro-F1/Macro-F1 ≈ 0.514

训练效率

耗时久（>2.5 小时），显存 8.2GB

耗时短，显存 4.5GB（轻量化优势显著）

4.4 结果分析

- 性能差异成因：LoRA 仅训练少量适配器参数，BERT 主干语义编码能力未充分适配任务；训练样本规模小于原论文全量数据；后期存在轻微过拟合

- 核心价值：验证了 LoRA 在混合架构中的适配性，实现了资源受限环境下的高效训练，为后续优化提供方向

🌟 项目创新点

1. 技术融合创新：将 LoRA 低秩适配技术与 BERT-BiLSTM-TextCNN 混合架构精准融合，突破全量微调的单一范式

2. 参数效率优化：仅训练 4.9780% 的低秩参数，大幅降低算力依赖，适配资源受限环境

3. 训练策略适配：结合分层学习率、余弦退火调度、混合精度训练，实现轻量化模式下的高效收敛

4. 实践空白填补：验证了 LoRA 在 BERT 混合架构中的适配性，为类似任务的轻量化落地提供参考

📌 后续优化方向

1. 超参数精细化：优化 LoRA 低秩维度、缩放因子，调整学习率调度的暖启动比例与下降速率

2. 抗过拟合优化：增加数据增强（同义词替换、文本扰动），优化 dropout 比例与 L2 正则化系数

3. 模型能力强化：尝试 LoRA 与其他轻量化技术（如 Adapter）的结合，进一步提升特征捕捉精度

4. 数据扩充：引入更多领域的短文本情感数据，提升模型泛化能力

📎 相关链接

- 原论文：《BERT-BiLSTM-TextCNN 模型的网民情感分类研究》

- 数据集：simplifyweibo_4_moods

- 代码仓库：https://github.com/aneo2024/NLP-course-assignment

📝 备注

本项目实验结果基于 Kaggle T4 GPU 环境完成，不同硬件环境下的训练效率可能存在差异，建议根据实际资源调整 batch_size、梯度累积步数等参数。
