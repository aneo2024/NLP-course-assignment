# BERT-BiLSTM-TextCNN + LoRA 短文本情感三分类
对网民评论进行情感分析，能够精准把握网民心理，降低网络舆论带来的风险。本项目聚焦短文本情感三分类任务，复现论文《BERT-BiLSTM-TextCNN 模型的网民情感分类研究》核心融合架构，并针对 Kaggle 资源约束引入 **LoRA 低秩适配微调技术** 实现轻量化训练。

**代码地址**：github账号

## 1. 项目简介
### 1.1 研究任务
本项目面向**社交媒体短文本情感三分类**，以微博评论为研究对象，基于公开数据集 `simplifyweibo_4_moods` 开展实验：
- 数据集包含 36 万+带情感标注的微博，原始含 4 种情感（喜悦 20 万条、愤怒/厌恶/低落各 5 万条）。
- 经数据校验后调整为**三分类任务**（喜悦、愤怒、厌恶），解决原数据集中厌恶与低落类别文本重复问题。
- 核心目标：在 Kaggle 资源受限环境下，实现模型高效训练与部署，为网络舆论分析提供技术支撑。

### 1.2 相关研究
短文本情感分类主流方案分为两类，各有优劣：
| 模型类型 | 代表模型 | 优势 | 劣势 |
|----------|----------|------|------|
| 单一神经网络模型 | TextCNN | 训练快、捕捉局部特征 | 无上下文语义依赖建模能力，处理反讽/倒装文本效果差 |
| | BiLSTM/TextRNN | 捕捉上下文时序特征 | 训练慢、局部特征提取能力弱 |
| | Word2Vec+单一模型 | 实现简单 | 词向量无动态上下文语义，分类准确率受限 |
| 混合神经网络模型 | BERT-BiLSTM-TextCNN | 融合语义向量+时序特征+局部特征，分类精度高 | 全量微调参数量大（BERT-base 超 1 亿参数），算力要求高 |

### 1.3 现有方案总结
本项目基于论文核心架构，结合**轻量化微调技术**构建解决方案：
1. 模型架构：BERT-BiLSTM-TextCNN 混合架构
2. 优化策略：引入 LoRA 低秩适配微调 + L2=1 强正则化
3. 数据处理：`simplifyweibo_4_moods` 去重 → 均衡抽样 → 按 7:2:1 划分训练/验证/测试集
4. 核心目标：平衡**高准确率、低参数量、快训练速度**，适配资源受限环境

### 1.4 算法改进动机
1. **适配资源受限环境**：原架构全量微调算力需求高，Kaggle 平台训练时长超 2.5 小时，需轻量化技术降低成本。
2. **保留核心架构优势**：BERT-BiLSTM-TextCNN 的三重特征融合能解决短文本语义浓缩、反讽识别等问题，无需重构架构。
3. **平衡性能与效率**：现有方案存在“精度优先”或“效率优先”的单一倾向，需实现双重目标的平衡。
4. **验证轻量化技术适配性**：LoRA 在混合架构中的应用尚未充分验证，本项目填补该实践空白。

## 2. 项目实现方案
### 2.1 任务的形式化描述
#### 输入空间
- 数据集：预处理后的微博短文本集合 $X=\{x_1,x_2,...,x_N\}$，单样本 $x_i=[w_{i1},w_{i2},...,w_{ik}]$（$w_{ij}$ 为词元，$k$ 为文本长度）。
- 数据处理：去重、均衡抽样后，通过 BERT 分词器转换为固定长度（max_len=64）的输入 ID 和注意力掩码。

#### 输出空间
- 情感标签集合 $Y=\{y_1,y_2,...,y_N\}$，标签空间 $C=\{0,1,2\}$ 对应**喜悦、愤怒、厌恶**三类情感。
- 模型输出：样本属于各类别的概率分布 $y_i=[p_{i0},p_{i1},p_{i2}]$，满足 $\sum_{c=0}^{2}p_{ic}=1$。

#### 模型映射关系
构建分类模型 $f:X→Y$，流程如下：
1. BERT 编码：将文本 $x_i$ 转换为 768 维上下文语义向量 $h_i \in \mathbb{R}^d$。
2. 特征融合：BiLSTM 捕捉时序特征，TextCNN 提取局部特征，拼接得到融合特征。
3. 分类预测：全连接层将融合特征映射为标签概率分布。

#### 损失函数
采用**交叉熵损失函数**衡量预测分布与真实标签的差异：
$$
\mathcal{L}=-\frac{1}{N}\sum_{i=1}^{N}\sum_{c=0}^{2}I(y_i=c)\log p_{ic}
$$
其中 $I(\cdot)$ 为指示函数，条件成立时取值为 1，否则为 0。

### 2.2 任务实现框架
#### 2.2.1 实验环境
- 硬件：Kaggle T4 GPU
- 软件：PyTorch 框架、`bert-base-chinese` 预训练模型、LoRA 相关依赖库
- 核心优势：保证实验可复现性与资源受限环境的适配性

#### 2.2.2 项目模块设计
整体流程遵循 **数据预处理 → 模型构建 → 训练优化 → 评估验证** 端到端流程。

##### （1）数据集划分
1. **去重处理**：发现原数据集“厌恶（label=2）”与“低落（label=3）”文本完全重合，保留喜悦、愤怒、厌恶三类，设置 `NUM_CLASSES=3`。
2. **均衡抽样**：对去重后 30 万条数据分层抽样 30%，再以最少样本数为基准均衡抽样，确保每类样本数均为 15508 条。
3. **数据集划分**：按 7:2:1 比例生成 `train_set.csv`、`val_set.csv`、`test_set.csv`，存储于 `/kaggle/working/` 目录。

##### （2）数据预处理模块
1. **文本清洗**：去除网址、@用户、话题 #、非中文字符及多余空格，保留中文文本与常用标点。
2. **标签映射**：原始标签 ID（0/1/2）映射为“喜悦/愤怒/厌恶”，保证数据集标签体系一致。
3. **数据封装**：自定义 `SentimentDataset` 类，基于 `torch.utils.data.Dataset` 实现数据加载，适配模型输入格式。

##### （3）模型架构
本项目模型由 **BERT-LoRA 嵌入层 + BiLSTM 层 + TextCNN 层 + 分类层** 组成：
1. **BERT-LoRA 嵌入层**
   - BERT 功能：生成 768 维上下文语义向量，输出形状 $[batch\_size, max\_len, 768]$。
   - LoRA 配置：插入到 BERT 的 query/key/value/dense 模块，低秩维度 $r=16$，缩放因子 `lora_alpha=64`，dropout=0.05，任务类型设为 `TaskType.FEATURE_EXTRACTION`。
   - 核心优势：冻结 BERT 主干参数，仅训练低秩适配器（可训练参数占比不足 5%），降低算力消耗。

2. **BiLSTM 层**
   - 输入：BERT 输出的 768 维隐藏态。
   - 配置：单层双向 LSTM，隐藏层维度 300，取最后时间步输出得到 600 维时序特征向量。
   - 功能：捕捉文本上下文时序依赖（如“虽然…但是…”的情感转折）。

3. **TextCNN 层**
   - 输入：BERT 隐藏态转置为 $[batch, 768, 64]$。
   - 配置：多尺度一维卷积核（3/4/5），最大池化提取局部关键词特征（如“开心”“气死”）。
   - 输出：拼接多尺度卷积结果，得到 300 维局部特征向量。

4. **分类层**
   - 输入：BiLSTM 时序特征与 TextCNN 局部特征的融合向量。
   - 功能：全连接层映射到 3 个情感类别，输出分类概率。

## 3. 实验结果
### 3.1 评估方式
#### （1）评估指标
选取 **Precision（精确率）、Recall（召回率）、F1 值** 作为基础指标，同时引入：
- **Micro-F1**：汇总所有类别 TP/FP/FN 计算，反映模型整体分类准确率。
- **Macro-F1**：计算各类别 F1 值的平均值，避免大类样本主导评估结果。

#### （2）训练配置
- 批量大小：300
- 优化器：AdamW
- 学习率策略：分层学习率（LoRA 层 2e-3，BiLSTM/TextCNN/全连接层 1e-3）
- 训练策略：混合精度训练 + 梯度累积（步数 4） + 余弦退火学习率调度（暖启动占比 10%）
- 训练轮数：30 轮

### 3.2 实验结果分析
#### （1）训练过程可视化特征
训练过程的损失与 Macro-F1 变化呈现以下特点：
- **损失曲线**：训练损失持续下降并趋于平缓，验证损失小幅波动但无显著上升，验证了 LoRA 轻量化微调的有效性与正则化策略的合理性。
- **Macro-F1 曲线**：训练集 Macro-F1 稳步上升；验证集 Macro-F1 整体上升，但在第 10 轮左右出现“下降-回升”拐点，属于模型特征泛化能力调整的阶段性现象。

#### （2）拐点成因分析
验证集 Macro-F1 拐点的出现源于三方面因素：
1. **学习率调度影响**：第 10 轮处于余弦退火学习率快速下降区间，参数更新节奏变化导致泛化性能短暂波动。
2. **LoRA 特征学习阶段**：前 10 轮适配基础情感特征，第 10 轮左右开始拟合反讽、倒装等复杂样本，特征泛化性不足导致性能回落。
3. **验证集分布特性**：验证集复杂样本占比高于训练集，模型对复杂样本的特征认知尚未稳定。

#### （3）与原论文结果对比
| 对比维度 | 原论文 | 本项目 |
|----------|--------|--------|
| 任务类型 | 四分类（喜悦/愤怒/厌恶/低落） | 三分类（喜悦/愤怒/厌恶） |
| 训练范式 | BERT 全量微调 | BERT-LoRA 轻量化微调 |
| 核心指标 | Micro-F1=0.901，Macro-F1=0.908 | 测试集 Micro-F1=0.5141，Macro-F1=0.5140 |
| 类别性能 | - | 喜悦类 F1=0.5628 最优，厌恶类 F1=0.4866 最弱 |
| 训练效率 | Kaggle T4 训练超 2.5 小时，显存 8.2GB | 30 轮训练耗时大幅降低，显存降至 4.5GB |

**性能差异原因**：
1. 本项目仅训练 LoRA 适配器（可训练参数占比不足 5%），BERT 主干语义编码能力未充分适配任务。
2. 训练样本规模相对缩减，且验证集在第 20 轮后出现轻微过拟合。
3. 轻量化微调的核心优势是降低算力消耗，性能牺牲属于合理trade-off。

## 4. 项目总结
### 4.1 项目创新
1. **技术融合创新**：首次将 LoRA 低秩适配技术与 BERT-BiLSTM-TextCNN 混合架构结合，突破全量微调的单一范式，可训练参数占比降至 4.9780%。
2. **训练策略优化**：设计分层学习率、混合精度训练、余弦退火调度的协同策略，在 Kaggle T4 GPU 上完成 30 轮轻量化训练，验证了技术落地可行性。
3. **数据处理优化**：解决原数据集类别文本重复问题，通过均衡抽样保证数据集类别平衡，提升模型泛化能力。

### 4.2 项目启发与未来方向
#### （1）核心启发
- 微调的核心是**参数效率**而非参数数量，轻量化微调更符合实际应用的资源约束需求。
- LoRA 适配混合架构需系统性优化超参数（低秩维度、学习率、正则化系数），避免“拟合快、泛化弱”问题。

#### （2）未来优化方向
1. **超参数精细化调整**：提升 LoRA 低秩矩阵维度，优化学习率调度的暖启动比例与下降速率。
2. **缓解过拟合**：增加数据增强策略（同义词替换、随机裁剪），调整 dropout 比例与 L2 正则化系数。
3. **模型能力强化**：尝试融合注意力机制，增强模型对情感关键词的聚焦能力；探索更大规模预训练模型的轻量化适配。

## 5. 免责声明
本项目仅用于学术研究，数据集 `simplifyweibo_4_moods` 来源于公开渠道，若涉及版权问题请联系作者处理。
